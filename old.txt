import os
import logging
import nltk
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from chromadb import Client
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import HuggingFaceHub
from langchain_core.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from dotenv import load_dotenv
from transformers import AutoConfig
# Cấu hình logging và NLTK
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
nltk.download('punkt')

# Load biến môi trường
load_dotenv()

# Đường dẫn đến dataset
dataset_path = './dataset.txt'
if not os.path.isfile(dataset_path):
    raise FileNotFoundError(f"{dataset_path} does not exist.")

# Tải và xử lý dữ liệu
loader = TextLoader(dataset_path, encoding='utf-8')
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=4)
docs = text_splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings()

# Cấu hình Chroma
index_name = "infinity-demo"
persist_directory = "./chroma_data"
if not os.path.exists(persist_directory):
    os.makedirs(persist_directory)

if not os.access(persist_directory, os.W_OK):
    raise PermissionError(f"The directory {persist_directory} is not writable.")

chroma_client = Client(Settings(persist_directory=persist_directory))
existing_collections = [collection.name for collection in chroma_client.list_collections()]

if index_name not in existing_collections:
    docsearch = Chroma.from_documents(documents=docs, embedding=embeddings, collection_name=index_name, persist_directory=persist_directory)
else:
    docsearch = Chroma(collection_name=index_name, persist_directory=persist_directory)

docsearch.persist()

# Cấu hình LLM
repo_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"
config = AutoConfig.from_pretrained(repo_id)
max_sequence_length = config.max_position_embeddings

llm = HuggingFaceHub(
    repo_id=repo_id,
    model_kwargs={"temperature": 0.6, "top_k": 20, "top_p": 0.85, "max_length": min(10000, max_sequence_length)},
    huggingfacehub_api_token="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"
)

# Tạo prompt template
template = """
Act as an expert in everything and answer the question of users. 
Your response should be limited to 2 sentences or 1000 words and summarize the main points of the context.
Answer will be translated to Vietnamese.
IMPORTANT: summarize the main points of the context.

Context: {context}
Question: {question}
Answer: 
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# Tạo RAG chain
rag_chain = (
    {"context": docsearch.as_retriever(), "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
def save_to_file(question, answer):
    with open('new_answers.txt', 'a', encoding='utf-8') as f:
        f.write(f"Q: {question}\nA: {answer}\n\n")


def chat():
    while True:
        user_input = input("Nhập câu hỏi của bạn (hoặc 'quit' để thoát): ")
        if user_input.lower() == 'quit':
            break
        try:
            result = rag_chain.invoke(user_input)   
            answer_start = "Answer: "
            response = result.split(answer_start)[-1].strip()
            print("Câu trả lời:", response)
            save_to_file(user_input, response)
        except Exception as e:
            logger.error(f"Lỗi: {e}")
            print(f"Đã xảy ra lỗi: {e}")

if __name__ == '__main__':
    chat()
